{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easyocr in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: torch in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (2.7.0)\n",
      "Requirement already satisfied: torchvision>=0.5 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (0.22.0)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (2.2.5)\n",
      "Requirement already satisfied: Pillow in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (11.2.1)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (0.25.2)\n",
      "Requirement already satisfied: python-bidi in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (0.6.6)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (6.0.2)\n",
      "Requirement already satisfied: Shapely in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (2.1.0)\n",
      "Requirement already satisfied: pyclipper in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from easyocr) (1.11.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from torch->easyocr) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from torch->easyocr) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from torch->easyocr) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from torch->easyocr) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from torch->easyocr) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from torch->easyocr) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from torch->easyocr) (80.4.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from scikit-image->easyocr) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from scikit-image->easyocr) (2025.3.30)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from scikit-image->easyocr) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from scikit-image->easyocr) (0.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alban nyantudre\\desktop\\github\\frame2text4llm\\.venv\\lib\\site-packages (from jinja2->torch->easyocr) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#%pip install easyocr tqdm scipy opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from scipy.fft import dct\n",
    "from difflib import SequenceMatcher #find similarity between 2 strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"sample_short.mp4\"\n",
    "sample_rate= 1 #frames per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  0.0,\n",
       "  array([[168, 168, 168, ..., 106,  99,  90],\n",
       "         [168, 168, 168, ..., 115,  99,  96],\n",
       "         [168, 168, 168, ..., 128, 103, 101],\n",
       "         ...,\n",
       "         [111, 110, 110, ..., 149, 144, 142],\n",
       "         [111, 111, 111, ..., 148, 145, 144],\n",
       "         [111, 111, 111, ..., 145, 145, 145]], shape=(900, 720), dtype=uint8)),\n",
       " (30,\n",
       "  1.0,\n",
       "  array([[224, 181, 124, ..., 103, 103, 102],\n",
       "         [212, 167, 110, ..., 106, 106, 104],\n",
       "         [185, 141,  84, ..., 109, 110, 110],\n",
       "         ...,\n",
       "         [ 58,  58,  57, ..., 146, 146, 146],\n",
       "         [ 57,  57,  57, ..., 146, 146, 146],\n",
       "         [ 56,  56,  56, ..., 146, 146, 146]], shape=(900, 720), dtype=uint8)),\n",
       " (60,\n",
       "  2.0,\n",
       "  array([[ 71,  71,  71, ..., 195, 195, 195],\n",
       "         [ 71,  71,  71, ..., 194, 194, 194],\n",
       "         [ 71,  71,  71, ..., 192, 192, 192],\n",
       "         ...,\n",
       "         [ 81,  81,  81, ..., 200, 200, 200],\n",
       "         [ 82,  82,  82, ..., 199, 199, 199],\n",
       "         [ 82,  82,  82, ..., 199, 199, 199]], shape=(900, 720), dtype=uint8))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_frames(video_path, sample_rate=2):\n",
    "    \"\"\"yield (frame_no, timestamp_sec, gray_frame).\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    step = max(1, int(fps / sample_rate))\n",
    "    frame_no = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_no % step == 0:\n",
    "            ts = frame_no / fps\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            yield frame_no, ts, gray\n",
    "        frame_no += 1\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "frames = list(extract_frames(video_path, sample_rate))\n",
    "frames[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(900, 720)\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "print(type(frames[0][2]))\n",
    "print(frames[0][2].shape)\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learned some new things here while looking for methods to filter duplicate images: Discrete Cosine Transform (was thinking about FOurier tranform). \n",
    "- DCT concentrates most of the image's \"energy\" (structural information) into the top-left coefficients of the transformed matrix.\n",
    "- FFT distributes energy more evenly across the frequency spectrum, making it harder to isolate key low-frequency components.\n",
    "\n",
    "The DCT is the core of JPEG compression, computationally efficient and numerically stable making it ideal for perceptual hashing, where the goal is to extract stable, low-frequency features while ignoring noise and minor distortions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  0.0,\n",
       "  array([[168, 168, 168, ..., 106,  99,  90],\n",
       "         [168, 168, 168, ..., 115,  99,  96],\n",
       "         [168, 168, 168, ..., 128, 103, 101],\n",
       "         ...,\n",
       "         [111, 110, 110, ..., 149, 144, 142],\n",
       "         [111, 111, 111, ..., 148, 145, 144],\n",
       "         [111, 111, 111, ..., 145, 145, 145]], shape=(900, 720), dtype=uint8)),\n",
       " (30,\n",
       "  1.0,\n",
       "  array([[224, 181, 124, ..., 103, 103, 102],\n",
       "         [212, 167, 110, ..., 106, 106, 104],\n",
       "         [185, 141,  84, ..., 109, 110, 110],\n",
       "         ...,\n",
       "         [ 58,  58,  57, ..., 146, 146, 146],\n",
       "         [ 57,  57,  57, ..., 146, 146, 146],\n",
       "         [ 56,  56,  56, ..., 146, 146, 146]], shape=(900, 720), dtype=uint8)),\n",
       " (60,\n",
       "  2.0,\n",
       "  array([[ 71,  71,  71, ..., 195, 195, 195],\n",
       "         [ 71,  71,  71, ..., 194, 194, 194],\n",
       "         [ 71,  71,  71, ..., 192, 192, 192],\n",
       "         ...,\n",
       "         [ 81,  81,  81, ..., 200, 200, 200],\n",
       "         [ 82,  82,  82, ..., 199, 199, 199],\n",
       "         [ 82,  82,  82, ..., 199, 199, 199]], shape=(900, 720), dtype=uint8))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _phash(image, hash_size=8, highfreq_factor=4):\n",
    "    \"\"\"computes a perceptual hash (fingerprint) of an image\"\"\"\n",
    "    img = cv2.resize(image, (hash_size * highfreq_factor,)*2)\n",
    "    d = dct(dct(img.astype(float), axis=0), axis=1)\n",
    "    low = d[:hash_size, :hash_size]\n",
    "    return (low > np.median(low)).flatten()\n",
    "\n",
    "def filter_duplicates(frames, diff_thresh=10):\n",
    "    kept, prev = [], None\n",
    "    for fn, ts, img in frames:\n",
    "        h = _phash(img)\n",
    "        if prev is None or np.count_nonzero(h != prev) > diff_thresh:\n",
    "            kept.append((fn, ts, img))\n",
    "            prev = h\n",
    "    return kept\n",
    "\n",
    "frames_filtered = filter_duplicates(frames)\n",
    "frames_filtered[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 720)\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "print(frames_filtered[0][2].shape)\n",
    "print(len(frames_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "OCR:   0%|          | 0/198 [00:00<?, ?it/s]c:\\Users\\Alban NYANTUDRE\\Desktop\\GitHub\\Frame2Text4LLM\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "OCR: 100%|██████████| 198/198 [01:50<00:00,  1.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.0, 'clideo com'),\n",
       " (30, 1.0, 'clideo com'),\n",
       " (60, 2.0, 'Chef de faille clideo com'),\n",
       " (90, 3.0, 'clideocom'),\n",
       " (120, 4.0, 'je vous ai appelé avez-vous eu des nouvelles de Rose clideo com'),\n",
       " (150, 5.0, 'je vous &i appelé avez-vous eu des nouvelles de Rose clideo com'),\n",
       " (180, 6.0, 'je vous &i appelé avez-vous eu des nouvelles de Rose clideo.com'),\n",
       " (210, 7.0, 'je vous ai appelé avez-vous eu des nouvelles de Rose clideo com'),\n",
       " (240, 8.0, \"Tu n'as pas de téléphone ? clideo com\"),\n",
       " (270, 9.0, \"Tu n'as pas de telephone ? clideo com\")]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _crop_subtitle_band(frame, height_ratio=0.2):\n",
    "    \"\"\"Crop bottom `height_ratio` of the frame.\"\"\"\n",
    "    h = frame.shape[0]\n",
    "    return frame[int(h * (1 - height_ratio)):, :]\n",
    "\n",
    "def ocr_frames(frames, langs=['fr'], gpu=False):\n",
    "    reader = easyocr.Reader(langs, gpu=gpu)\n",
    "    results = []\n",
    "    for fn, ts, img in tqdm(frames, desc=\"OCR\"):\n",
    "        crop = _crop_subtitle_band(img)\n",
    "        arr = np.array(crop)\n",
    "        try:\n",
    "            boxes = reader.readtext(arr)\n",
    "        except Exception:\n",
    "            print(f\"OCR error on frame {fn}: {e}\")\n",
    "            continue\n",
    "        text = ' '.join(b[1] for b in boxes).strip()\n",
    "        if text: #and not text.lower().endswith(\"clideo com\"):\n",
    "            results.append((fn, ts, text))\n",
    "    return results\n",
    "\n",
    "\n",
    "ocrs = ocr_frames(frames_filtered, gpu=True)\n",
    "ocrs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "print(len(frames_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def merge_segments(ocr_res, sim_thresh=0.8):\n",
    "    if not ocr_res:\n",
    "        return []\n",
    "    segs = []\n",
    "    cur_txt = ocr_res[0][2]\n",
    "    start, end = ocr_res[0][1], ocr_res[0][1]\n",
    "    for _, ts, txt in ocr_res[1:]:\n",
    "        if _similar(cur_txt, txt) >= sim_thresh: #if text == cur_text:\n",
    "            end = ts\n",
    "            # keep longer\n",
    "            if len(txt) > len(cur_txt):\n",
    "                cur_txt = txt\n",
    "        else:\n",
    "            segs.append((start, end, cur_txt))\n",
    "            cur_txt, start, end = txt, ts, ts\n",
    "    segs.append((start, end, cur_txt))\n",
    "    return segs\n",
    "\n",
    "segs = merge_segments(ocrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "OCR:   0%|          | 0/198 [00:00<?, ?it/s]c:\\Users\\Alban NYANTUDRE\\Desktop\\GitHub\\Frame2Text4LLM\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "OCR: 100%|██████████| 198/198 [02:03<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def _format_ts(sec):\n",
    "    ms = int((sec - int(sec)) * 1000)\n",
    "    h, rem = divmod(int(sec), 3600)\n",
    "    m, s = divmod(rem, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}.{ms:03d}\"\n",
    "\n",
    "def video_to_subs_json(video_path, sample_rate=2, gpu=True):\n",
    "    frames = list(extract_frames(video_path, sample_rate))\n",
    "    frames = filter_duplicates(frames)\n",
    "    ocrs = ocr_frames(frames, gpu=gpu)\n",
    "    segs = merge_segments(ocrs)\n",
    "    return [\n",
    "        {\"start_time\": _format_ts(s), \"end_time\": _format_ts(e), \"text\": t}\n",
    "        for s, e, t in segs\n",
    "    ]\n",
    "\n",
    "\n",
    "resultat = video_to_subs_json(\"sample_short.mp4\", sample_rate=1)\n",
    "with open(\"resultat_first_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resultat, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
